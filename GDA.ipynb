{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc665ca-98d3-4223-8b75-87a58244cde3",
   "metadata": {},
   "source": [
    "# Overview:\n",
    "We will be running \"Gaussian Discriminant Analysis\" on the breast_cancer dataset. This demonstration aims to show how the respective formulae apply for binary classification, as a generative learning algorithm.\n",
    "\n",
    "# Generative algorithms - Differences in working:\n",
    "1. Discriminant methods such as GLM's or Newton's method in Logistic Regression, aim to create a mapping $f:X \\rightarrow Y$, where X is the dataset input labels and Y is the dataset output labels. We do this by estimating the parameters used in determing $P(Y = y^{(i)} | \\, X = x^{(i)})$, such that the joint log-likelihood, $l(\\theta) = \\sum_{i = 1}^{m} log \\, (P \\,(Y = y^{(i)} | \\, X = x^{(i)} \\, ; \\theta))$ is maximised in terms of $\\theta$.\n",
    "\n",
    "2. Generative methods, on the other hand, aim to create a mapping $f : Y \\rightarrow X$, wherein we calculate the parameters used in determing $P(X = x^{(i)} | \\, Y = y^{(i)})$, such that the joint log-likelihood statement is replaced by the Bayes' expansion of the term $(P \\,(Y = y^{(i)} | \\, X = x^{(i)} \\, ; \\theta)) = \\frac{P(X = x^{(i)} | \\, Y = y^{(i)}) P(Y = y^{(i)})}{P(X = x^{(i)})}$ where $P(X = x^{(i)}) = P(X = x^{(i)} | \\, Y = 1)P(Y = 1) + P(X = x^{(i)} | \\, Y = 0)P(Y = 0)$, assuming Y is a binary variable.\n",
    "\n",
    "3. As a design choice, we assume $x^{(i)} \\in \\mathbb{R}^{n}$ such that $x^{(i)} \\sim \\mathcal{N} (\\vec{\\mu_{(y^{(i)})}}, \\, \\sum)$ (Multivariate Gaussian, where $\\sum \\in \\mathbb{R}^{n \\times n}$ and $\\mu \\in \\mathbb{R}^{n}$) and $y^{(i)} \\sim Bernoulli(\\phi)$ (where $\\phi \\in (0, 1)$). This would mean their distributions will be given by: $$P(X = x \\,| \\, Y = y) = \\frac{1}{(2 \\pi)^{\\frac{n}{2}} |\\sum|^{\\frac{1}{2}}} exp(-\\frac{1}{2}(x - \\vec{\\mu})^{T} (\\sum)^{-1} (x - \\vec{\\mu}))$$ where $\\sum$ is covariance matrix and $\\mu$ is the mean. For Y: $$P(Y = y) = \\phi^{y} (1 - \\phi)^{1 - y}$$ where $\\phi$ is the probability of y = 1.\n",
    "\n",
    "4. The design choice for continuous x being Gaussian distributed is just a general result of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e08492bf-12ad-47e8-a72f-00de9b0a88c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the main libraries + dataset.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1bb99efb-8a67-4565-93e1-d4bac5619894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569, 1)\n"
     ]
    }
   ],
   "source": [
    "#Fetching the values from the dataset.\n",
    "data = load_breast_cancer()\n",
    "X_data = data.data\n",
    "Y_data = data.target.reshape(-1, 1)\n",
    "#Verifying shapes.\n",
    "print(X_data.shape)\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00e6b81e-03c0-49f7-8bb6-40b53cd9fd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 30)\n",
      "(69, 30)\n",
      "(500, 1)\n",
      "(69, 1)\n"
     ]
    }
   ],
   "source": [
    "#Scaling the sample elements in the dataset.\n",
    "scaler = StandardScaler()\n",
    "X_data_scaled = scaler.fit_transform(X_data)\n",
    "#Slicing the dataset into two matrix parts.\n",
    "#We will use the first 500 samples to train our model's parameters, and the next 69 as queries to be used on the model.\n",
    "X = X_data_scaled[:500, :]\n",
    "X_queries = X_data_scaled[500:, :]\n",
    "Y = Y_data[:500, :]\n",
    "Y_queries = Y_data[500:, :]\n",
    "#Verifying the shapes.\n",
    "print(X.shape)\n",
    "print(X_queries.shape)\n",
    "print(Y.shape)\n",
    "print(Y_queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8731128a-4884-4b26-ad60-56c8c11eb6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195, 30)\n",
      "(305, 30)\n"
     ]
    }
   ],
   "source": [
    "#Now, we fetch all samples with outputs 0 and 1 respectively.\n",
    "y = Y.flatten()\n",
    "#We need to convert Y into a 1D array for boolean indexing. \n",
    "X_0 = X[y == 0]\n",
    "X_1 = X[y == 1]\n",
    "#Verifying the shapes.\n",
    "print(X_0.shape)\n",
    "print(X_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e57131d-bba4-4e9e-bde2-719d9a948c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 30)\n",
      "(1, 30)\n",
      "(30, 30)\n",
      "(30, 30)\n"
     ]
    }
   ],
   "source": [
    "#Now, finding the parameters u_1 and u_0, or the means:\n",
    "u_1 = np.mean(X_1, axis = 0, keepdims = True)\n",
    "u_0 = np.mean(X_0, axis = 0, keepdims = True)\n",
    "#Now finding phi:\n",
    "phi = X_1.shape[0] / X.shape[0]\n",
    "#Now finding the covariance matrix:\n",
    "u_data = np.mean(X, axis = 0, keepdims = True)\n",
    "X_dev = X - u_data\n",
    "sigma = np.zeros((X.shape[1], X.shape[1]))\n",
    "#Verifying shapes.\n",
    "print(X_dev.shape)\n",
    "print(u_data.shape)\n",
    "print(sigma.shape)\n",
    "#Summing over all deviation matrices per sample and calculating mean accordingly\n",
    "i = 0\n",
    "#Set iterator at 0.\n",
    "while(i < X.shape[0]):\n",
    "    #Fetching ith row from the X_dev matrix.\n",
    "    X_dev_i = X_dev[[i], :]\n",
    "    #Calculating the deviation matrix for this row.\n",
    "    sig_i = X_dev_i.T @ X_dev_i\n",
    "    #Summing it up.\n",
    "    sigma += sig_i\n",
    "    i += 1\n",
    "#Factoring in X.shape[0] so as to get a mean value.\n",
    "sigma /= X.shape[0]\n",
    "#Verifying shape.\n",
    "print(sigma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "acb81196-6359-4374-a1f3-744dd2e224fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, using these parameters, we define the Probability Distributions required.\n",
    "#X_q is the qth query from the query matrix.\n",
    "def gauss_0(X_q):\n",
    "    return np.exp(-(X_q - u_0) @ np.linalg.pinv(sigma) @ (X_q - u_0).T / 2) / (((2 * math.pi)**(X.shape[1] / 2)) * ((np.linalg.det(sigma))**(0.5)))\n",
    "    #Multivariate gaussian for samples with output labels as 0.\n",
    "def gauss_1(X_q):\n",
    "    return np.exp(-(X_q - u_1) @ np.linalg.pinv(sigma) @ (X_q - u_1).T / 2) / (((2 * math.pi)**(X.shape[1] / 2)) * ((np.linalg.det(sigma))**(0.5)))\n",
    "    #Multivariate gaussian for sample with output labels as 1.\n",
    "def p_of_x(X_q):\n",
    "    #We return P(x) here, expanded by Bayes' decomposition rule.\n",
    "    return gauss_0(X_q)*(1 - phi) + gauss_1(X_q)*phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9bf204ce-a40c-4877-9468-6dba38b86986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "97.10144927536231\n"
     ]
    }
   ],
   "source": [
    "#Now, we iterate through the query space, and record the accuracy of our trained model.\n",
    "q = 0\n",
    "#Set iterator at 0.\n",
    "#We will use a counter to record the no. of correct predictions while iterating through our query space.\n",
    "acc_cnt = 0\n",
    "while(q < X_queries.shape[0]):\n",
    "    #Slicing the sample query.\n",
    "    X_q = X_queries[[q], :]\n",
    "    #Fetching conditional probability of x given y = 1.\n",
    "    p_y_1 = (gauss_1(X_q) * phi) / p_of_x(X_q)\n",
    "    #Since these are complimentary probabilities; p_y_0 = 1 - p_y_1.\n",
    "    p_y_0 = 1 - p_y_1\n",
    "    #Basic boolean checking to see if prediction is correct.\n",
    "    if p_y_1 >= p_y_0:\n",
    "        if Y_queries[q, 0] == 1:\n",
    "            acc_cnt += 1\n",
    "    else:\n",
    "        if Y_queries[q, 0] == 0:\n",
    "            acc_cnt += 1\n",
    "    q += 1\n",
    "#Printing accuracy percentage.\n",
    "print(acc_cnt)\n",
    "print((acc_cnt / X_queries.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3d938-feb2-404d-860f-adddc8d5a536",
   "metadata": {},
   "source": [
    "Since, we have to find out $P(X = x \\, | \\, Y = 1)$ and $P(X = x \\, | \\, Y = 0)$ both, we create two different PDF's with two separate mean parameters given by $$ P(X = x \\, | \\, Y = 1) \\sim \\mathcal{N}(\\vec{\\mu_{1}}, \\sum) $$ for Y = 1 and $$ P(X = x \\, | \\, Y = 0) \\sim \\mathcal{N}(\\vec{\\mu_{0}}, \\sum) $$ for Y = 0. These 2 separate PDF's help in modelling out the conditional probability $P(X = x | Y = y)$.\n",
    "\n",
    "We assume that the covariance matrix stays the same in the 2 PDF's, which may not be true always. This is a simple design choice.\n",
    "\n",
    "Anyways, thank you for sticking around :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
