{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "d6ff83d7-3ffc-4855-9304-15d0ec339391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following demonstration aims to give an idea of how locally weighted regression is performed.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Importing all the basic libraries and classes for performing operations.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "#We will be using the california_housing_dataset for our demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "90624d95-017f-42d3-a429-4bf49e9aeba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing()\n",
    "X_raw = data.data\n",
    "Y = data.target.reshape(-1, 1)\n",
    "#Fetched all the datapoints in matrix form.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)\n",
    "#Performed standardised scaling on them to prevent one feature from dominating the other.\n",
    "m = X_scaled.shape[0]\n",
    "ones = np.ones((m, 1))\n",
    "X = np.hstack([ones, X_scaled])\n",
    "#Added the bias term to our standardised input matrix X_scaled, to finally form our main matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "39a90e4f-169b-4713-8b89-048d7ee37903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LWR requires external queries for prediction of values. However we do not have any such query space with their actual values available for our dataset.\n",
    "#Hence, for comparison purpose of accuracy, we will be using query_space = X_input_space. Allowing us to measure the average accuracy.\n",
    "#For seeing the average accuracy, we will use the following formula:\n",
    "# 100 - [|Y_predicted - Y_real| * 100 / Y_real] = percentage of accuracy (assuming this to be equivalent to 1 - percentage of error)\n",
    "X_query_raw = X_raw[:, [1, 2]]\n",
    "#Copying the columns of raw X data to create our query space.\n",
    "X_query = scaler.fit_transform(X_query_raw)\n",
    "#Scaled the Query matrix to be used for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "5d6f6b9f-8ef7-4f23-8db4-0537fd0b4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, [1, 2]]\n",
    "#Since we only need 2 features of the X matrix, we will get rid of all other features.\n",
    "def weight_compute(X_query_i, tau):\n",
    "    diff = X - X_query_i\n",
    "    eucl_dist_sq = np.sum(diff**2, axis = 1)\n",
    "    #Calculating the euclidean distance between the query vector and the dataset input vector.\n",
    "    weights = np.exp(-eucl_dist_sq / (2 * tau**2))\n",
    "    #Using the basic gaussian weight formula, we make a 1-D array of weights to be attached to our datapoints per query.\n",
    "    return weights\n",
    "#Computed the Weight array to be used in our calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "474a949f-ea03-44b7-b331-9cf6a9bd1246",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[277], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m M \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack([X0\u001b[38;5;241m*\u001b[39mX0, X0\u001b[38;5;241m*\u001b[39mX1, X1\u001b[38;5;241m*\u001b[39mX1])\n\u001b[0;32m     17\u001b[0m A, B, C \u001b[38;5;241m=\u001b[39m M\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m weights_i\n\u001b[1;32m---> 18\u001b[0m term0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(Y_weighted_i\u001b[38;5;241m*\u001b[39mX0)\n\u001b[0;32m     19\u001b[0m term1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(Y_weighted_i\u001b[38;5;241m*\u001b[39mX1)\n\u001b[0;32m     20\u001b[0m D \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m*\u001b[39mC \u001b[38;5;241m-\u001b[39m B\u001b[38;5;241m*\u001b[39mB\n",
      "File \u001b[1;32m~\\Desktop\\Anaconda\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2172\u001b[0m, in \u001b[0;36m_sum_dispatcher\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2103\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[0;32m   2104\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2167\u001b[0m \n\u001b[0;32m   2168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m, a_min, a_max, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 2172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2173\u001b[0m                     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[0;32m   2177\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[0;32m   2178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2179\u001b[0m         initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "#Iterator through our query space. Gives the ith query.\n",
    "tau = 0.1\n",
    "#Hyperparameter set at 0.1 for now, aka the standard variance for gaussian weight.\n",
    "predicted_values = []\n",
    "#Array to store our predicted values per query.\n",
    "while(i < 200):\n",
    "    X_query_i = X_query[i]\n",
    "    #Fetching ith row as our query vector.\n",
    "    weights_i = weight_compute(X_query_i, tau)\n",
    "    #Computing array of weights.\n",
    "    Y_weighted_i = Y * weights_i[:, np.newaxis] \n",
    "    Y_Weighted_i = Y_weighted_i.reshape(-1)\n",
    "    X0 = X[:, 0]\n",
    "    X1 = X[:, 1]\n",
    "    M = np.column_stack([X0*X0, X0*X1, X1*X1])\n",
    "    A, B, C = M.T @ weights_i\n",
    "    term0 = np.sum(Y_weighted_i*X0)\n",
    "    term1 = np.sum(Y_weighted_i*X1)\n",
    "    D = A*C - B*B\n",
    "    Q1 = (C*term0 - B*term1)/D\n",
    "    Q2 = (A*term1 - B*term0)/D\n",
    "    Q_i = np.array([Q1,Q2]).reshape(2,1)\n",
    "    #This is a simplified version of the standard locally weighted regression formula.\n",
    "    #The formula is normally given by (X.T @ W @ X)^-1 @ X.T @ W @ Y. The time complexity is approximately O(m^2).\n",
    "    #Here, W is the diagonal matrix of shape (m x m) of weights given per query for each datapoint input of the form [[w1, 0, 0, 0....], [0, w2, 0, 0...]..[0, 0.. 0, wm]]\n",
    "    #My simplified version of formula and the standard formula can be verified to be equal. The simplified version has a complexity of O(m).\n",
    "    H = X_query_i @ Q_i\n",
    "    #Calculating the predicted value for one query.\n",
    "    predicted_values.append(H)\n",
    "    #Storing the value for future plot graphing.\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "eb1468ae-2da0-46bd-9021-4ce715381378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.0068950653076171875\n"
     ]
    }
   ],
   "source": [
    "#This Cell was written by chatGPT.\n",
    "\n",
    "import time\n",
    "# Prepare a single query\n",
    "x_q = X_query[0]\n",
    "start = time.time()\n",
    "# 1) weights\n",
    "weights_i = weight_compute(x_q, tau)              # O(m)\n",
    "Yw = Y.ravel() * weights_i                 # O(m)\n",
    "X0, X1 = X[:,0], X[:,1]\n",
    "# 2) A, B, C in one pass\n",
    "M = np.column_stack([X0*X0, X0*X1, X1*X1])        # (m,3)\n",
    "A,B,C = M.T @ weights_i                          # single O(m) matrixâ€‘vector\n",
    "# 3) Q1, Q2\n",
    "term0 = np.sum(Yw*X0)                             # O(m)\n",
    "term1 = np.sum(Yw*X1)                             # O(m)\n",
    "Q1 = C*term0 - B*term1\n",
    "Q2 = A*term1 - B*term0\n",
    "Q     = np.array([Q1,Q2]).reshape(2,1)           # O(1)\n",
    "# 4) single prediction\n",
    "y_pred = x_q @ Q                                  # O(n)=O(2)\n",
    "print(\"Elapsed:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10883c-8fb7-4d8c-b8de-76e93dccb8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
