{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1afbf7bf-fbbd-4c68-a566-0aa6aa8c1710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This implementation for Locally Weighted Regression aims to demonstrate its application for estimating values of actual Non-Linear graphs for a given query.\n",
    "#The algorithm uses the concept of Gaussian weights, which determine the \"importance\" or \"weight\" of any given C_i due to the ith sample. \n",
    "#This helps in approximating a linear function that is approximately equivalent to the true graph around a small neighbourhood of the query point.\n",
    "#The process is similar to computing normal equations for the analytical solution of theta in gradient descent with linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55942298-bc70-4036-9d48-ee5be1c46bd7",
   "metadata": {},
   "source": [
    "# Let's start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3fae9884-9f0f-4d5f-ab9f-36093d30d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#Importing all the basic libraries needed for operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04fb4720-2882-4ea3-8588-825a98b06a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(-10 * np.pi, 10 * np.pi, size = (2000, 10))\n",
    "Y = np.sum(np.sin(X), axis = 1) + 0.3 * np.sum(X**2, axis = 1) + 2 * np.random.randn(2000)\n",
    "#We will synthesise a dataset matrix X with 2000 samples (rows) and 10 features (columns).\n",
    "#Each entry of this matrix will range uniformly from [-10*pi, 10*pi]. \n",
    "#We also synthesise a random base function off of which our Y matrix will be centered.\n",
    "#To simulate approximate real-world conditions, we introduce gaussian noise to this as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64181966-895b-4393-8b0b-780c40b485ca",
   "metadata": {},
   "source": [
    "## Notes about the function choice:\n",
    "1. Our true function is randomly defined such that $\\vec{X} \\in \\mathbb{R}^{n}$ , where: $F_{true} (\\vec{X}) = \\sum_{j = 1}^{n} sin(x_{j}) + 0.3 \\sum_{j = 1}^{n} x_{j}^{2}$ where $x_{j}$ is the $j^{th}$ co-ordinate/entry of $\\vec{X}$ input row vector.\n",
    "2. Now, to simulate real world conditions, we introduce a gaussian noise to the true function, such that: $Y = F_{true} (\\vec{X}) + 2\\eta$ where $\\eta $ ~ $\\mathcal{N} (\\mu = 0, \\sigma^{2} = 1)$ , where Y is our target matrix.\n",
    "3. The purpose of making a non-linear function is satisfied here, along with simulating real world conditions.\n",
    "4. The constant and functions so chosen in $F_{true} (\\vec{X})$, the distribution of entries of X matrix being uniform are all design choices.\n",
    "5. However, the noise being distributed under a Gaussian PMF is a much needed condition for LWR to hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d38a22b4-7897-41d1-a45a-cf5f687330df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 11)\n",
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "one = np.ones((2000, 1))\n",
    "X = np.hstack([one, X])\n",
    "Y = Y.reshape(-1, 1)\n",
    "#Adding the bias term to account for the noise as accurately as possible.\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "#Verifying the shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "34163abe-c0e0-40f7-bf6f-04004cfac5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 11)\n",
      "(20, 1)\n"
     ]
    }
   ],
   "source": [
    "X_query = X[::100]\n",
    "Y_verify = Y[::100]\n",
    "#We defined every 100 entries in the X matrix to function as our query matrix. This is a design choice.\n",
    "#This will lead to a total of 20 sample queries in our query space, whose predicted values can be verified from the Y matrix as well.\n",
    "print(X_query.shape)\n",
    "print(Y_verify.shape)\n",
    "#Verifying the query space shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b3d180a-29f2-486a-83c0-f4d7e425a01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1034.83638286]]\n",
      "1034.836382859744\n",
      "[[693.87624651]]\n",
      "961.9687332346251\n",
      "[[1145.96585291]]\n",
      "786.7616819608722\n",
      "[[1036.97734137]]\n",
      "702.086632319787\n",
      "[[725.27894244]]\n",
      "1105.6598426702196\n",
      "[[1293.94132898]]\n",
      "983.3773784247517\n",
      "[[409.1240999]]\n",
      "1036.4219147651527\n",
      "[[797.38110532]]\n",
      "591.8716023021316\n",
      "[[666.70064946]]\n",
      "818.6951240628097\n",
      "[[956.01522303]]\n",
      "598.0437458889023\n",
      "[[1230.62628786]]\n",
      "849.858598158439\n",
      "[[1885.24798861]]\n",
      "930.3466800847214\n",
      "[[1167.60889969]]\n",
      "1092.1756762599293\n",
      "[[858.59751177]]\n",
      "398.44230322080625\n",
      "[[821.71045268]]\n",
      "682.4890249265949\n",
      "[[377.4045915]]\n",
      "821.8556131198612\n",
      "[[410.24378108]]\n",
      "1383.215765647978\n",
      "[[681.04603873]]\n",
      "635.9669528481256\n",
      "[[1020.85345562]]\n",
      "334.52408324840536\n",
      "[[482.94046779]]\n",
      "695.4770178383808\n"
     ]
    }
   ],
   "source": [
    "q = 0\n",
    "#q is the query iterator.\n",
    "tau = 1\n",
    "#Assuming a standard deviation of 1 along the gaussian weights. Arbitrarily chosen.\n",
    "Y_preds = []\n",
    "#An array/list to store the predicted values.\n",
    "while(q < 20):\n",
    "    X_q = X_query[[q], :]\n",
    "    #Fetching the X_q query vector as a row.\n",
    "    weights_q = np.exp(-np.sum((X - X_q)**2, axis = 1, keepdims = True) / (2 * tau**2))\n",
    "    #Making a column vector of weights. Instead of using direct matrix multiplication, we will use vectorized multiplication in our methods.\n",
    "    X_weighted = X * weights_q\n",
    "    Y_weighted = Y * weights_q\n",
    "    #These two are essential components that the inverse and the usual term in the closed form formula comprise of.\n",
    "    Q = np.linalg.pinv(X.T @ X_weighted) @ (X.T @ Y_weighted)\n",
    "    #The above 4 statements demonstrate an easier method to calculate the close form solution for Theta, without the need of a W matrix.\n",
    "    #This formula yields the same result as the general closed-form solution.\n",
    "    print(X_q @ Q)\n",
    "    print(Y[q, 0])\n",
    "    #Manually make comparisons between the predicted and actual values for the query. \n",
    "    Y_preds.append(X_q @ Q)\n",
    "    #Record the predicted values per query.\n",
    "    q += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0c99296-c700-40e6-b965-2afeb94ba8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.268478079503412e-26\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y_verify, np.array(Y_preds).reshape(-1, 1)))\n",
    "#Checking our mean-squared error between the actual and predicted values for the query points. \n",
    "#The closer this value is to 0, the more the accuracy.\n",
    "#Y_preds had earlier been defined as a simple python array. To reshape it into a matrix, we need to broadcast it into a numpy array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79273ec3-267a-4ddc-86c6-426bcc566bff",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "1. As we can see, the value for our mean squared error, $MSE = \\sum_{q = 1}^{20} \\frac{Y_{real}^{q} - Y_{predicted}^{q}}{20}$ is close to 0 (in the order of $10^{-26}$ ).\n",
    "2. This yields from the analytical closed form solution for our parameter vector $\\vec{\\theta^{*}}$ such that $\\theta^{q} = (X^{T}WX)^{-1} (X^{T}WY)$ where W is the square diagonal matrix of the shape $m$ x $m$, such that the $i^{th}$ row contains the gaussian weight $\\omega^{i}_{q}$ for the $q^{th}$ query at position $[W_{ii}]$.\n",
    "3. Here, the gaussian weight $\\omega^{i}_{q} = exp(-\\frac{||X^{i} - X^{q}||_{2}^{2}}{2 \\tau^{2}})$ where $X^{i}$ is the $i^{th}$ sample and the $X^{q}$ is the $q^{th}$ query such that $X^{i}, X^{q} \\in \\mathbb{R}^{n}$.\n",
    "4. Now, we have used a simplified formula for this: $\\vec{\\theta^{*}} = (X^{T}X_{weighted})^{-1} (X^{T}Y_{weighted})$, where $X_{weighted} = X * weights$ and $Y_{weighted} = Y * weights$, where \"weights\" is the column vector of shape $m$ x $1$, with $i^{th}$ element given by $\\omega^{i}_{q}$.\n",
    "5. The formula might not make a difference mathematically. But measuring in terms of time complexity, the original closed form solution has $O(nm^{2})$ time complexity, whereas our improved form has complexity $O(mn^{2})$. Considering the fact that we have set (no. of features n) < (no. of samples m), vectorized operations in our improved form significantly improve computation speed for m >> 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
