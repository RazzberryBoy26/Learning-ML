{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7d89de-b21c-4fac-95bd-6ebdd7f22203",
   "metadata": {},
   "source": [
    "# Overview:\n",
    "Would like to share my experience regarding the implementation of the algorithm for locally weighted regression. This is actually a failed implementation, since there are some errors I was not able to resolve at all. Would like it if you guys can guide me through.\n",
    "\n",
    "## Some mistakes, and what I learned from them:\n",
    "1. At first, tried using the whole $\\omega$ matrix for the computation of $\\vec{\\theta}$ parameter matrix. It was a failed attempt due to huge computation time issues. So, manually solved the formula equation to get my final scalar, and made some vectorised operations to replace the need for $\\omega$ matrix.\n",
    "2. Even before this, thought of implementing batch gradient descent per query. After running for one query, quickly realised that it is not feasible in terms of computation power and such. \n",
    "    **Nevertheless, here's my formula for updation of $\\vec{\\theta}$ using bgd :**\n",
    "    $\\vec{\\theta}^{t+1} = \\vec{\\theta}^{t} - \\alpha\\nabla_{\\theta}$ $\\sum_{i=1}^m \\frac{\\exp(\\frac{-||X^{i} - X^{q}||^{2}}{2\\tau^{2}})(Y^{i} - X^{i} \\vec{\\theta})^{2}}{2m}$ where $\\sum_{i=1}^m \\frac{\\exp(\\frac{-||X^{i} - X^{q}||^{2}}{2\\tau^{2}})(Y^{i} - X^{i} \\vec{\\theta})^{2}}{2m}$ = J($\\vec{\\theta}$).\n",
    "3. At first tried running whole python loops through matrices for squaring purpose and such. Now I use vectorized operations available via numpy for faster execution.\n",
    "4. Learned about the concept of Euclidean distances, and how it is implemented in the given algorithm. (The term $||X^{i} - X^{q}||^{2}$ is actually the euclidean scalar distance between the $i^{th}$ row vector of dataset, $\\vec{X^{i}}$, and the $q^{th}$ query row vector $\\vec{X^{q}}$.) \n",
    "5. At first I was running an infinite while loop, solely because I didn't put i += 1 at the end of the loop. Kept wondering what went wrong for a long time ;-;\n",
    "\n",
    "## What I fail to understand:\n",
    "In my given .ipynb file, there are 2 cells that I have used for execution of my main algorithm. One was written by me, and the other one was written by chatGPT. After a bit of analysis, I found that the time complexities of both the execution codes are essentially the same, i.e. O(mn), where m is no. of elements in dataset and n is no. of features per input (which I set to be 2 for the given case). The chatGPT block of code executes one query in approximately an order of $10^{-2}$ or $10^{-3}$ seconds. I personally wanted to run my overall algorithm for 200 such queries and with going by simple logic, the total time for execution should not exceed more than 2 or 3 seconds. However, even though the complexities of both the codes per query are same, the time doesn't match, as my code takes WAY longer to execute than it should. ChatGPT says it is about me running too many python loops in comparison to C loops (while doing vectorized operations), but such a HUGE difference seems unlikely.\n",
    "\n",
    "Thank you for sticking around (even for this failed one) :,)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
