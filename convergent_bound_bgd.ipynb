{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77cd24ae-e37e-41d8-ac65-db2b9d097b59",
   "metadata": {},
   "source": [
    "# What I aim to do:\n",
    "The batch gradient descent algorithm requires setting a suitable combination of the number of iterations(n) and the learning rate(α) to get our required convergence point over any convex quadratic cost function. To set these values is generally a matter of trial and error. Using the proposed algorithm, we may be able to bind the analytic convergence point for any parameter between two values. This allows us to get an estimate of where the convergence point might lie, and henceforth, giving us a rough idea for setting 'n' and 'α' suitably. The overall trial and error process will be sped up many fold.\n",
    "\n",
    "# Important assumptions:\n",
    "1. The cost function J so chosen, is convex.\n",
    "2. It has a single global minimum to move towards.\n",
    "\n",
    "# The Algorithm:\n",
    "1. Consider any one particular feature of $\\vec{\\theta}$ vector, $\\theta_{j}$ ​. We initialize $\\theta_{j}$ = 0 $\\forall$ j $\\in$ [0, N], where N is number of features. Our basic updation algorithm remains the same, that is: $\\theta^{t + 1}_{j}$ = $\\theta^{t}_{j}$ - $\\alpha$ $\\frac{\\partial J(\\vec{\\theta}^{t})}{\\partial \\theta_{j}}$, where 't' is the sequence counter.\n",
    "\n",
    "2. Since the cost function J($\\vec{\\theta}$) so chosen is convex and has a single global minimum, our updation shall take place monotonically in the direction of our analytic convergence value, i.e. the value of $\\theta_{j}$ will either monotonically increase or monotonically decrease with increasing t.\n",
    "\n",
    "3. Considering the purpose in hand, I would like to speed up the process for selection of '$\\alpha$' and 'n'. Let's initialise 'n' and '$\\alpha$' with some arbitrary values. Upon setting these up, there can be 3 possible cases:\n",
    "    1. The $\\alpha$ and n are too small to approach the limit, sluggish movements are seen with each t.\n",
    "    2. The $\\alpha$ and n are too large to approach the limit, movements oscillate with large amplitude around the convergence point with each t.\n",
    "    3. The $\\alpha$ and n are very much suitable to approach the limit, and good convergence is seen with each t.\n",
    "\n",
    "4. To get an interval matrix which contains the limit for $\\vec{\\theta}$ vector, we consider the direction of approach/the trajectory of $\\theta_{j}$ $\\forall$ j $\\in$ [0, N] individually. Plotting a graph between sequence counter t and $\\theta^{t}_{j}$, we see that for initial t, the direction of $\\theta_{j}$ is monotonic (either purely increasing or decreasing, as established already in point 2).\n",
    "    **The direction of $\\theta_{j}$ at some t is defined as $\\theta^{t + 1}_{j}$ - $\\theta^{t}_{j}$**\n",
    "\n",
    "5. To find a bound for the limit of $\\theta_{j}$, we will find the first occurence of sign change.\n",
    "    **This matters because this $t = l^{th}$ sequence counter will now serve as our first bounding value.**\n",
    "    1. **Why the sign change?**\n",
    "    We know that in the initial phase of the sequence, a monotonic behaviour was observed. But, in the next few iterations it might so happen that the value of $\\theta_{j}$ exceeds (if initially monotonically increasing) or goes under the limit point (if initially monotonically decreasing) due to a large learning rate. This would cause the $\\theta_{j}$ to overshoot the convergent limit. However, since the sequence ultimately converges to the limit (mathematically proven), it would cause the direction of $\\theta_{j}$ to change sign, to again \"try and go back\" to the convergent point. This will give us one bounding value for the limit point.\n",
    "\n",
    "6. To find the next bound after this, we will have to find the next occurence of sign change.\n",
    "    **This next occurence at $t = r^{th}$ sequence counter will now serve as our second bounding value.**\n",
    "    1. **Again, why the sign change?**\n",
    "    If the second sign change is observed, it means that again $\\theta_{j}$ overshot the convergence point. Which means it would cause the direction of $\\theta_{j}$ to again change sign and \"try and go back\" to the convergent point. This will give us another bounding value for the limit point.\n",
    "\n",
    "    2. **Why the second occurence?**\n",
    "    After the first occurence, the second occurence will change the sign back to the original sign, causing an overall oscillatory behaviour around the convergent limit. The bounds of these oscillations will serve as our bound for the limit point.\n",
    "\n",
    "7. These two values will serve as our upper and lower bounds on the convergent limit. To view a graphical explanation, click here -->[].\n",
    "**Note : X - axis denotes t(sequence counter) and Y - axis denotes $\\theta_{j}$.**\n",
    "\n",
    "8. Now let's go through each case given in point 3 and see what happens:\n",
    "\n",
    "    ## Case 1. The $\\alpha$ and n are too small to approach the limit, sluggish movements are seen with each t.\n",
    "    1. This means that the sign change will never occur in the first place, as $\\vec{\\theta}$ is approaching the limit point very slowly starting from $\\vec{0}$. Hence, possibility of overshooting the convergent point at all is negligible.\n",
    "    2.  This means that no sign change will occur. Hence no upper bound is obtained. This will give us an indication that '$\\alpha$' or 'n' might be too small, prompting us to increase it accordingly.\n",
    "\n",
    "    ## Case 2. The $\\alpha$ and n are too large to approach the limit, movements oscillate with large amplitude around the convergence point with each t.\n",
    "    1. This means that either 'n' is set too small relative to '$\\alpha$' or '$\\alpha$' is set too large relative to 'n'. This would cause $\\vec{\\theta}$ to oscillate violently about the convergent point. \n",
    "    2. This would be seen by the size of the interval as recorded by the algorithm. If the size of the interval is very large, it demonstrates a larger range for the convergent point. For our purpose, we would want the convergence range to be as small as possible. If we decrease '$\\alpha$' or increase 'n', this will be possible.\n",
    "\n",
    "    ## Case 3. The $\\alpha$ and n are very much suitable to approach the limit, and good convergence is seen with each t.\n",
    "    1. This would mean either a very small or no interval has been formed. Hence, convergence is at par with the limit point.\n",
    "\n",
    "    ## Case 4?? There are more than two sign changes in the overall sequence formation.\n",
    "    1. This would mean that $\\vec{\\theta}$ overshot the limit point more than twice. If the overshooting is more than twice, then a more accurate bound can be formed by the next possible sign changes; since with each iteration, the difference between the sequence and the limit becomes less. \n",
    "    2. This would mean, we want to find the last possible pair of iterations with sign changing from same to opposite to same. For this, we will run an O(n) while loop, whilst trying to record the possible pair combinations, and updating them if needed.\n",
    "\n",
    "...........................................................................................................................................................\n",
    "\n",
    "\n",
    "# Well, I tried running it:\n",
    "Weird stuff happened that I either didn't understand or just overlooked. \n",
    "## Some of my mistakes:\n",
    "1. The time complexity is not O(n) but O(n*N) instead, in terms of running C loops.\n",
    "\n",
    "2. Unexpected behaviour occured when $\\alpha$ was large. Instead of going for a damped oscillating trajectory about $\\theta^{*}$, the $\\vec{\\theta}$ vector started making violent oscillations about the limit point. There is something about hessians and eigenvalues that I don't know in data optimisation theory, as per the explanation offered by ChatGpt.\n",
    "\n",
    "3. I often had to change the dimensions of matrices in my code writing (code available at: https://github.com/RazzberryBoy26/Learning-ML/blob/Week-2/Bound_convergence_demo.ipynb). Again, these silly mistakes will just go away with practice I guess :,)\n",
    "\n",
    "## What did I learn though?\n",
    "1. At first, I thought of running while loops inside the matrices to record my slope updates for each $\\theta_{j}$. But this would make the time complexity go up to O(n.$N^{2}$), which could easily blow up under high number of features, and make the program unfeasible to run. This prompted me to learn commands for vectorized calculations, which requires performing arithmetic/bit-wise operations over all elements of an array using super-fast C loops. These were commands that helped performing individual entry operations on vectors with a much faster time complexity. It made the program much faster.\n",
    "\n",
    "2. This one is not technical; rather philosophical I would say. As I keep on learning new concepts, it makes me want to try and apply them in novel ways to see if something beneficial results. And although the \"beneficial results\" will not be as common, I certainly do learn many new techniques and operations that were previously unknown to me before during the process itself. Sometimes, just trying matters in the end, because whatever you go through will essentially teach you something - be it something small, or even life-changing. Just a random thought I had :p\n",
    "\n",
    "## So, will I try this project ever again?\n",
    "Certainly! From whatever I have gathered, this project may be feasible in the long run as I keep on learning new things. I will be postponing this project for now, but will certainly try to add value to it in case I learn something that may help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22323330-4b26-4991-bbcd-bcd6d56a0daf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
