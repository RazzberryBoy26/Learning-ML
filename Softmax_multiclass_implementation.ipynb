{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b453d82-431c-4d0a-b400-f695ec17cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This implementation demonstrates how Softmax multiclassification works for datasets with linearly separable classes more than 2.\n",
    "#The implementation uses the load_iris dataset for the demonstration. 3 distinct classes are available here.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Importing all the standard libraries + dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c55a700f-bbf4-4854-b32e-70560a72108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X_raw = data.data\n",
    "Y_raw = data.target\n",
    "#Fetching the dataset.\n",
    "Y = data.target.reshape(-1, 1)\n",
    "#Converting Y into matrix form for further operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4882350-5121-4121-b61f-ff9e3fe8f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)\n",
    "#Scaling the X matrix for overflow prevention.\n",
    "one = np.ones((X_scaled.shape[0], 1))\n",
    "X = np.hstack([one, X_scaled])\n",
    "#Adding the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ce8c973-1fe6-4573-9000-5cb27d1c4b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n",
      "(150, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "#Verifying the shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c7effff-40a6-4722-b031-31840d6a1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((np.unique(Y.flatten()).size, X.shape[1]))\n",
    "#Initialising the theta tensor with 0's.\n",
    "def grad(pred_stat, X_i, Y_i):\n",
    "    M1 = pred_stat.T\n",
    "    main_term = M1 @ X_i\n",
    "    #Using the predicted one-hot-vector and target one-hot-vector, we try to produce the gradient. \n",
    "    main_term[[Y_i], :] -= X_i\n",
    "    #Minimising the distance between the two.\n",
    "    return main_term\n",
    "#Returning the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72f27d8c-996c-412b-ada8-79386fd17d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.66666666666667\n",
      "84.66666666666667\n",
      "84.66666666666667\n",
      "84.66666666666667\n",
      "85.33333333333334\n",
      "84.66666666666667\n",
      "84.66666666666667\n",
      "84.66666666666667\n",
      "84.66666666666667\n",
      "84.66666666666667\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "#Initialised epoch iterator at 0.\n",
    "epochs = 10\n",
    "#Arbitrarily set no. of epochs.\n",
    "alpha = 0.001\n",
    "#Arbitrarily set learning rate.\n",
    "while(j < epochs):\n",
    "    #While loop start.\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    #Shuffling the X matrix everytime we go through an epoch for improved accuracy.\n",
    "    for i in indices:\n",
    "        #For loop for iterating through all the inputs.\n",
    "        X_i = X[[i], :]\n",
    "        #Storing the ith input vector.\n",
    "        logit_i = X_i @ Q.T\n",
    "        #Creating the logit space for further operations.\n",
    "        logit_i -= np.max(logit_i)\n",
    "        #A small mathematical manipulation. This does not affect the result, which can be verified mathematically.\n",
    "        pred_stat = np.exp(logit_i)/np.sum(np.exp(logit_i))\n",
    "        #Getting our predicted one-hot-vector with probabilistic entries.\n",
    "        Y_i = Y[i, 0]\n",
    "        #Fetching the index of true output.\n",
    "        Q -= alpha*grad(pred_stat, X_i, Y_i)\n",
    "        #Basic updation rule.\n",
    "    j += 1\n",
    "    #Going to the next epoch.\n",
    "    H = Q @ X.T\n",
    "    #Making a linear function of X and theta for getting predicted values.\n",
    "    preds = np.argmax(H, axis = 0)\n",
    "    #Intuitively, maximum value indicates highest probability. No need to convert into [0, 1] space.\n",
    "    preds = preds.reshape(-1, 1)\n",
    "    #Converting into matrix form.\n",
    "    correct = (preds == Y)\n",
    "    #Making another matrix to check no. of correct predictions.\n",
    "    accuracy = np.mean(correct) * 100\n",
    "    print(accuracy)\n",
    "    #Printing accuracy of predictions on overall dataset per epoch.\n",
    "    alpha *= 0.95\n",
    "    #Decay constant for gradient updation. Prevents noisiness each epoch.\n",
    "#Implementing while loop to iterate through epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6b3dc-4610-49b3-86ba-38a118ee3bcc",
   "metadata": {},
   "source": [
    "# Overview:\n",
    "This is a standard implementation of the softmax regression for classifying multiple classes using the cross-entropy function. Multiclassification problems are very much a part of real world problems, and this demonstration goes in depth with how the math works in it: all the code and basic implementation practices that are essential.\n",
    "# Learning, mistakes, doubts:\n",
    "1. Learned a vectorized C-loop command (\"np.unique(Y).size\") to note the total no. of unique classes possible. Crucial for making the one-hot-vector.\n",
    "2. The first command requires flattening the matrix first. It only works with 1-D arrays.\n",
    "3. This was a small suggestion by chatGPT, which I later verified mathematically as well. Subtracting the max entry of the logit space from all entries of that space prevents overflow without changing the mathematical result.\n",
    "4. Learned about a new argmax command to fetch maximum element from matrices using C-loop vectorization.\n",
    "\n",
    "Thank you for sticking around :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
